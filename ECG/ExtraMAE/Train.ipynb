{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from Model import Patchify, ExtraMAEDecoder, ExtraMAEEncoder, ExtraMAE\n",
    "from util import random_indexes, take_indexes\n",
    "from einops import repeat, rearrange\n",
    "import matplotlib.pyplot as plt\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import ProjectConfiguration\n",
    "\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "\n",
    "from diffusion.util import instantiate_from_config\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusion.util import instantiate_from_config, load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRS = ['checkpoints', 'logs', 'samples', \"final\", \"model\"]\n",
    "logger = get_logger(__name__, log_level=\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"name\": \"MITBIH-ExtraMAE\",\n",
    "    \"exp_dir\": \"/home/bejar/PycharmProjects/misiones/Series/Models/ExtraMAE/Train\",\n",
    "    \"ema\":{\n",
    "        \"inv_gamma\": 1.0,\n",
    "        \"power\": 0.75,\n",
    "        \"max_decay\":0.9999\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"in_channels\": 1,\n",
    "        \"series_length\": 192,\n",
    "        \"mask_percent\": 0.75,\n",
    "        \"layers\":8,\n",
    "        \"heads\":4,\n",
    "        \"embed_dim\":32,\n",
    "        \"patch_size\":16\n",
    "    },\n",
    "\n",
    "    \"projectconf\": {\n",
    "        \"total_limit\": 2\n",
    "    },\n",
    "    \"accelerator\": {\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"mixed_precision\": \"no\",\n",
    "        \"log_with\":\"wandb\"\n",
    "    },\n",
    "    \"optimizer\":{\n",
    "        \"beta1\":0.95,\n",
    "        \"beta2\":0.999,\n",
    "        \"weight_decay\":1e-6,\n",
    "        \"epsilon\":1e-08\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"lr_warmup_steps\": 100,\n",
    "        \"epochs\": 10,\n",
    "        \"checkpoint_freq\": 2000,\n",
    "        \"checkpoint_epoch_freq\": 2,\n",
    "        \"loss\": \"L2\"\n",
    "    },\n",
    "    \"samples\": {\n",
    "        \"samples_freq\": 25,\n",
    "        \"samples_num\": 20,\n",
    "        \"samples_gen\": 1000\n",
    "    },\n",
    "\n",
    "    \"dataset\": {\n",
    "        \"name\": \"EBHI\",\n",
    "        \"nclasses\":5,\n",
    "        \"train\": {\n",
    "            \"class\":\"ai4ha.data.series.MITBIHDataLoader.MITBIHtrain\",\n",
    "            \"params\":{\n",
    "                \"filename\": \"/home/bejar/ssdstorage/MITBIH/mitbih_train.csv\",\n",
    "                \"n_samples\": 2000,\n",
    "                \"resamp\": False,\n",
    "                \"oneD\": True,\n",
    "                \"fixsize\": 192,\n",
    "                \"normalize\": False\n",
    "            }\n",
    "        },\n",
    "\n",
    "        \"test\":{ \n",
    "            \"class\": \"ai4ha.data.series.MITBIHDataLoader.MITBIHtest\",\n",
    "            \"params\":{\n",
    "                \"filename\": \"/home/bejar/ssdstorage/MITBIH/mitbih_test.csv\",\n",
    "                \"n_samples\": 100,\n",
    "                \"resamp\": False,\n",
    "                \"oneD\": True,\n",
    "                \"fixsize\": 192,\n",
    "                \"normalize\": False\n",
    "            }\n",
    "        },\n",
    "        \"dataloader\":{\n",
    "            \"batch_size\": 512,\n",
    "            \"num_workers\": 6,\n",
    "            \"shuffle\": True\n",
    "        }\n",
    "    },\n",
    "    \"time\": 12\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(\"configs/ExtraMAE-KUHAR-L8-H4-E64-M75-LR4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bejar/env/torch13/lib/python3.10/site-packages/accelerate/accelerator.py:371: UserWarning: `log_with=wandb` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = f\"{config['exp_dir']}/logs/{config['name']}\"\n",
    "\n",
    "for dir in DIRS:\n",
    "    os.makedirs(f\"{BASE_DIR}/{dir}\", exist_ok=True)   \n",
    "\n",
    "accparams = config['accelerator']\n",
    "# accparams[\"logging_dir\"] = f\"{BASE_DIR}/logs\"\n",
    "accparams[\"project_dir\"] = BASE_DIR\n",
    "\n",
    "if 'projectconf' in config:\n",
    "    accparams['project_config'] = ProjectConfiguration(**config['projectconf'])\n",
    "\n",
    "ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=accparams['gradient_accumulation_steps'] > 1)\n",
    "accelerator = Accelerator(**accparams, kwargs_handlers=[ddp_kwargs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/22/2024 07:34:38 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger.info(accelerator.state, main_process_only=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1, 64]) 300 15\n"
     ]
    }
   ],
   "source": [
    "model=  ExtraMAE(\n",
    "        in_channels=config['model']['in_channels'],\n",
    "        series_length=config['model']['series_length'],\n",
    "        mask_percent=config['model']['mask_percent'],\n",
    "        num_layers=config['model']['layers'],\n",
    "        num_heads=config['model']['heads'],\n",
    "        embed_dimension=config['model']['embed_dim'],\n",
    "        patch_size=config['model']['patch_size']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config['train']['learning_rate'] * accelerator.num_processes,\n",
    "    betas=(config['optimizer']['beta1'], config['optimizer']['beta2']),\n",
    "    weight_decay=config['optimizer']['weight_decay'],\n",
    "    eps=config['optimizer']['epsilon'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20750, 300)\n",
      "(20750, 300)\n",
      "(20750, 300)\n",
      "(20750, 300)\n",
      "(20750, 300)\n",
      "(20750, 300)\n",
      "X_train shape is (20750, 6, 300)\n",
      "y_train shape is (20750,)\n",
      "(20750, 300)\n",
      "(20750, 300)\n",
      "(20750, 300)\n",
      "(20750, 300)\n",
      "(20750, 300)\n",
      "(20750, 300)\n",
      "X_train shape is (20750, 6, 300)\n",
      "y_train shape is (20750,)\n"
     ]
    }
   ],
   "source": [
    "train_data = instantiate_from_config(config['dataset']['train'])\n",
    "test_data = instantiate_from_config(config['dataset']['test'])\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, **config['dataset'][\"dataloader\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = get_scheduler(\n",
    "    \"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config['train']['lr_warmup_steps'] * accparams['gradient_accumulation_steps'],\n",
    "    num_training_steps=(len(train_dataloader) * config['train']['epochs']),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, lr_scheduler)\n",
    "model.to(accelerator.device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4613872e0c424f1c8e6c2e55d052e626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49aa62729ab34ea0ad799e3cf3337091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e66f2d39da4d3dab30a3f53020825f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "378431d9f4704f19944681629f35dc94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd5c092aeba54e4789b9cc133b31fa47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612abc3046634e17abc2c484fcff006e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dfe9d33a796455fa1d3cc189ad2270a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a9bca9ac91d42b58294800543422017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c6663ac21e4b71a122bb96a242e726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1718d3c183574a778421d25e3e917da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_update_steps_per_epoch = len(train_dataloader) // config['accelerator']['gradient_accumulation_steps']\n",
    "model.train()\n",
    "for epoch in range(config['train']['epochs']):\n",
    "    progress_bar = tqdm(total=num_update_steps_per_epoch,\n",
    "                        disable=not accelerator.is_local_main_process)\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "    mean_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        with accelerator.accumulate(model):\n",
    "            data, label = batch\n",
    "            data = data.float().to(accelerator.device)\n",
    "            pred, mask = model(data)\n",
    "            batch_mask = data * mask\n",
    "            pred_mask = pred * mask\n",
    "            loss = torch.nn.functional.mse_loss(pred_mask,\n",
    "                                                batch_mask,\n",
    "                                                reduction=\"none\")\n",
    "            loss = loss.mean()\n",
    "            mean_loss += loss.item()\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_postfix({\"loss\": loss.item(), \"mean_loss\": mean_loss/(step+1)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata = train_data[2][0] #test_data[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 300)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttdata = torch.tensor(tdata).unsqueeze(0).to(accelerator.device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 300])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a = torch.empty(ttdata.shape[2] // config['model']['patch_size']).uniform_(0, 1)\n",
    "\n",
    "mask = torch.tensor([0,1,0,1,0,1,0,1,0,1,0,1])\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexes = torch.nonzero(mask - 1, as_tuple=False)\n",
    "# indexes_comp = torch.nonzero(mask, as_tuple=False)\n",
    "# indexes_r = torch.cat((indexes[0], indexes_comp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward_indexes = torch.as_tensor(np.stack([i[0] for i in indexes_r],\n",
    "#                                         axis=-1),\n",
    "#                                 dtype=torch.long)\n",
    "# forward_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = torch.nonzero(mask - 1, as_tuple=False)\n",
    "indexes_comp = torch.nonzero(mask, as_tuple=False)\n",
    "indexes_comp = torch.cat((indexes, indexes_comp), dim=0)\n",
    "forward_indexes = torch.as_tensor(np.stack([i[0] for i in indexes_comp], axis=-1),\n",
    "                                  dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward_indexes.shape, series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (12) must match the size of tensor b (20) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 14\u001b[0m\n\u001b[1;32m      7\u001b[0m features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\n\u001b[1;32m      8\u001b[0m     series,\n\u001b[1;32m      9\u001b[0m     model\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mmask_token\u001b[38;5;241m.\u001b[39mexpand(indexes_comp\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m series\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     10\u001b[0m                            series\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m ],\n\u001b[1;32m     12\u001b[0m                      dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     13\u001b[0m features \u001b[38;5;241m=\u001b[39m take_indexes(features, indexes_comp\u001b[38;5;241m.\u001b[39mto(accelerator\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[0;32m---> 14\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mfeatures\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_embedding_d\u001b[49m\n\u001b[1;32m     15\u001b[0m features \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mtrd_layer(features)\n\u001b[1;32m     16\u001b[0m features \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mhead(features)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (12) must match the size of tensor b (20) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "series = model.encoder.patchify(ttdata)\n",
    "series = rearrange(series, 'b c t -> t b c')\n",
    "series += model.encoder.pos_embedding\n",
    "series = take_indexes(series, indexes_comp.to(accelerator.device))\n",
    "series = series[:forward_indexes.shape[0]]\n",
    "series = model.encoder.tre_layer(series)\n",
    "features = torch.cat([\n",
    "    series,\n",
    "    model.decoder.mask_token.expand(indexes_comp.shape[0] - series.shape[0],\n",
    "                           series.shape[1], -1)\n",
    "],\n",
    "                     dim=0)\n",
    "features = take_indexes(features, indexes_comp.to(accelerator.device))\n",
    "features = features + model.decoder.pos_embedding_d\n",
    "features = model.decoder.trd_layer(features)\n",
    "features = model.decoder.head(features)\n",
    "features = model.decoder.patch2img(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(features[0][0].cpu().detach().numpy(), c='r')\n",
    "plt.plot(ttdata[0][0].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, mask = model(ttdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(res[0][0].cpu().detach().numpy(), c='r')\n",
    "plt.plot(ttdata[0][0].cpu().detach().numpy());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mask[0][0].cpu().detach().numpy(), c='r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
